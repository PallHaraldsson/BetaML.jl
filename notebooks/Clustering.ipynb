{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and collaborative filtering (via clustering) algorithms\n",
    "\n",
    "- [Importable source code (most up-to-date version)](https://github.com/sylvaticus/lmlj.jl/blob/master/src/clusters.jl) - [Julia Package](https://github.com/sylvaticus/lmlj.jl)\n",
    "- [Demonstrative static notebook](https://github.com/sylvaticus/lmlj.jl/blob/master/notebooks/clusters.ipynb)\n",
    "- [Demonstrative live notebook](https://mybinder.org/v2/gh/sylvaticus/lmlj.jl/master?filepath=notebooks%2Fclusters.ipynb) (temporary personal online computational environment on myBinder) - it can takes minutes to start with!\n",
    "- Theory based on [MITx 6.86x - Machine Learning with Python: from Linear Models to Deep Learning](https://github.com/sylvaticus/MITx_6.86x) ([Unit 4](https://github.com/sylvaticus/MITx_6.86x/blob/master/Unit%2004%20-%20Unsupervised%20Learning/Unit%2004%20-%20Unsupervised%20Learning.md))\n",
    "- New to Julia? [A concise Julia tutorial](https://github.com/sylvaticus/juliatutorial) - [Julia Quick Syntax Reference book](https://julia-book.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Random\n",
    "using Distributions\n",
    "using Statistics\n",
    "using DelimitedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×2 Array{Float64,2}:\n",
       " 1.0  10.5\n",
       " 1.5  10.8\n",
       " 1.8   8.0\n",
       " 1.7  15.0\n",
       " 3.2  40.0\n",
       " 3.6  32.0\n",
       " 3.3  38.0\n",
       " 5.1  -2.3\n",
       " 5.2  -2.4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "X = [1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initRepresentatives"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "  initRepresentatives(X,K;initStrategy,Z₀))\n",
    "\n",
    "Initialisate the representatives for a K-Mean or K-Medoids algorithm\n",
    "\n",
    "# Parameters:\n",
    "* `X`: a (N x D) data to clusterise\n",
    "* `K`: Number of cluster wonted\n",
    "* `initStrategy`: Wheter to select the initial representative vectors:\n",
    "  * `random`: randomly in the X space\n",
    "  * `grid`: using a grid approach [default]\n",
    "  * `shuffle`: selecting randomly within the available points\n",
    "  * `given`: using a provided set of initial representatives provided in the `Z₀` parameter\n",
    " * `Z₀`: Provided (K x D) matrix of initial representatives (used only together with the `given` initStrategy) [default: `nothing`]\n",
    "\n",
    "# Returns:\n",
    "* A (K x D) matrix of initial representatives\n",
    "\n",
    "# Example:\n",
    "```julia\n",
    "julia> Z₀ = initRepresentatives([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.6 38],2,initStrategy=\"given\",Z₀=[1.7 15; 3.6 40])\n",
    "```\n",
    "\"\"\"\n",
    "function initRepresentatives(X,K;initStrategy=\"grid\",Z₀=nothing)\n",
    "    (N,D) = size(X)\n",
    "    # Random choice of initial representative vectors (any point, not just in X!)\n",
    "    minX = minimum(X,dims=1)\n",
    "    maxX = maximum(X,dims=1)\n",
    "    Z = zeros(K,D)\n",
    "    if initStrategy == \"random\"\n",
    "        for i in 1:K\n",
    "            for j in 1:D\n",
    "                Z[i,j] = rand(Uniform(minX[j],maxX[j]))\n",
    "            end\n",
    "        end\n",
    "    elseif initStrategy == \"grid\"\n",
    "        for d in 1:D\n",
    "                Z[:,d] = collect(range(minX[d], stop=maxX[d], length=K))\n",
    "        end\n",
    "    elseif initStrategy == \"given\"\n",
    "        if isnothing(Z₀) error(\"With the `given` strategy you need to provide the initial set of representatives in the Z₀ parameter.\") end\n",
    "        for d in 1:D\n",
    "                Z = Z₀\n",
    "        end\n",
    "    elseif initStrategy == \"shuffle\"\n",
    "        for d in 1:D\n",
    "            zIdx = shuffle(1:size(X)[1])[1:K]\n",
    "            Z = X[zIdx, :]\n",
    "        end\n",
    "    else\n",
    "        error(\"initStrategy \\\"$initStrategy\\\" not implemented\")\n",
    "    end\n",
    "    return Z\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 1.0   8.0\n",
       " 3.6  40.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z₀ = initRepresentatives([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.6 38],2,initStrategy=\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeans"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic K-Means Algorithm (Lecture/segment 13.7 of https://www.edx.org/course/machine-learning-with-python-from-linear-models-to)\n",
    "\n",
    "\"\"\"\n",
    "  kmeans(X,K,initStrategy)\n",
    "\n",
    "Compute K-Mean algorithm to identify K clusters of X using Euclidean distance\n",
    "\n",
    "# Parameters:\n",
    "* `X`: a (N x D) data to clusterise\n",
    "* `K`: Number of cluster wonted\n",
    "* `initStrategy`: Wheter to select the initial representative vectors:\n",
    "  * `random`: randomly in the X space\n",
    "  * `grid`: using a grid approach [default]\n",
    "  * `shuffle`: selecting randomly within the available points\n",
    "  * `given`: using a provided set of initial representatives provided in the `Z₀` parameter\n",
    " * `Z₀`: Provided (K x D) matrix of initial representatives (used only together with the `given` initStrategy) [default: `nothing`]\n",
    "\n",
    "# Returns:\n",
    "* A tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n",
    "\n",
    "# Notes:\n",
    "* Some returned clusters could be empty\n",
    "\n",
    "# Example:\n",
    "```julia\n",
    "julia> (clIdx,Z) = kmeans([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3)\n",
    "```\n",
    "\"\"\"\n",
    "function kmeans(X,K;initStrategy=\"grid\",Z₀=nothing)\n",
    "    (N,D) = size(X)\n",
    "    # Random choice of initial representative vectors (any point, not just in X!)\n",
    "    minX = minimum(X,dims=1)\n",
    "    maxX = maximum(X,dims=1)\n",
    "    Z₀ = initRepresentatives(X,K,initStrategy=initStrategy,Z₀=Z₀)\n",
    "    Z  = Z₀\n",
    "    cIdx_prev = zeros(Int64,N)\n",
    "\n",
    "    # Looping\n",
    "    while true\n",
    "        # Determining the constituency of each cluster\n",
    "        cIdx      = zeros(Int64,N)\n",
    "        for (i,x) in enumerate(eachrow(X))\n",
    "            cost = Inf\n",
    "            for (j,z) in enumerate(eachrow(Z))\n",
    "               if (norm(x-z)^2  < cost)\n",
    "                   cost    =  norm(x-z)^2\n",
    "                   cIdx[i] = j\n",
    "               end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Determining the new representative by each cluster\n",
    "        #for (j,z) in enumerate(eachrow(Z))\n",
    "        for j in  1:K\n",
    "            Cⱼ = X[cIdx .== j,:] # Selecting the constituency by boolean selection\n",
    "            Z[j,:] = sum(Cⱼ,dims=1) ./ size(Cⱼ)[1]\n",
    "        end\n",
    "\n",
    "        # Checking termination condition: clusters didn't move any more\n",
    "        if cIdx == cIdx_prev\n",
    "            return (cIdx,Z)\n",
    "        else\n",
    "            cIdx_prev = cIdx\n",
    "        end\n",
    "\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2, 2, 2, 3, 3, 3, 1, 1], [5.15 -2.3499999999999996; 1.5 11.075; 3.366666666666667 36.666666666666664])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clIdx,Z) = kmeans([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cos_distance"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic K-Medoids Algorithm (Lecture/segment 14.3 of https://www.edx.org/course/machine-learning-with-python-from-linear-models-to)\n",
    "\n",
    "\"\"\"Square Euclidean distance\"\"\"\n",
    "square_euclidean(x,y) = norm(x-y)^2\n",
    "\n",
    "\"\"\"Cosine distance\"\"\"\n",
    "cos_distance(x,y) = dot(x,y)/(norm(x)*norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmedoids"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "  kmedoids(X,K;dist,initStrategy,Z₀)\n",
    "\n",
    "Compute K-Medoids algorithm to identify K clusters of X using distance definition `dist`\n",
    "\n",
    "# Parameters:\n",
    "* `X`: a (n x d) data to clusterise\n",
    "* `K`: Number of cluster wonted\n",
    "* `dist`: Function to employ as distance (must accept two vectors). Default to squared Euclidean.\n",
    "* `initStrategy`: Wheter to select the initial representative vectors:\n",
    "  * `random`: randomly in the X space\n",
    "  * `grid`: using a grid approach\n",
    "  * `shuffle`: selecting randomly within the available points [default]\n",
    "  * `given`: using a provided set of initial representatives provided in the `Z₀` parameter\n",
    " * `Z₀`: Provided (K x D) matrix of initial representatives (used only together with the `given` initStrategy) [default: `nothing`]\n",
    "\n",
    "# Returns:\n",
    "* A tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n",
    "\n",
    "# Notes:\n",
    "* Some returned clusters could be empty\n",
    "\n",
    "# Example:\n",
    "```julia\n",
    "julia> (clIdx,Z) = kmedoids([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,dist = (x,y) -> norm(x-y)^2,initStrategy=\"grid\")\n",
    "```\n",
    "\"\"\"\n",
    "function kmedoids(X,K;dist=(x,y) -> norm(x-y)^2,initStrategy=\"shuffle\",Z₀=nothing)\n",
    "    (n,d) = size(X)\n",
    "    # Random choice of initial representative vectors\n",
    "    Z₀ = initRepresentatives(X,K,initStrategy=initStrategy,Z₀=Z₀)\n",
    "    Z = Z₀\n",
    "    cIdx_prev = zeros(Int64,n)\n",
    "\n",
    "    # Looping\n",
    "    while true\n",
    "        # Determining the constituency of each cluster\n",
    "        cIdx      = zeros(Int64,n)\n",
    "        for (i,x) in enumerate(eachrow(X))\n",
    "            cost = Inf\n",
    "            for (j,z) in enumerate(eachrow(Z))\n",
    "               if (dist(x,z) < cost)\n",
    "                   cost =  dist(x,z)\n",
    "                   cIdx[i] = j\n",
    "               end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Determining the new representative by each cluster (within the points member)\n",
    "        #for (j,z) in enumerate(eachrow(Z))\n",
    "        for j in  1:K\n",
    "            Cⱼ = X[cIdx .== j,:] # Selecting the constituency by boolean selection\n",
    "            nⱼ = size(Cⱼ)[1]     # Size of the cluster\n",
    "            if nⱼ == 0 continue end # empty continuency. Let's not do anything. Stil in the next batch other representatives could move away and points could enter this cluster\n",
    "            bestCost = Inf\n",
    "            bestCIdx = 0\n",
    "            for cIdx in 1:nⱼ      # candidate index\n",
    "                 candidateCost = 0.0\n",
    "                 for tIdx in 1:nⱼ # target index\n",
    "                     candidateCost += dist(Cⱼ[cIdx,:],Cⱼ[tIdx,:])\n",
    "                 end\n",
    "                 if candidateCost < bestCost\n",
    "                     bestCost = candidateCost\n",
    "                     bestCIdx = cIdx\n",
    "                 end\n",
    "            end\n",
    "            Z[j,:] = reshape(Cⱼ[bestCIdx,:],1,d)\n",
    "        end\n",
    "\n",
    "        # Checking termination condition: clusters didn't move any more\n",
    "        if cIdx == cIdx_prev\n",
    "            return (cIdx,Z)\n",
    "        else\n",
    "            cIdx_prev = cIdx\n",
    "        end\n",
    "\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2, 2, 2, 3, 3, 3, 1, 1], [5.1 -2.3; 1.5 10.8; 3.3 38.0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clIdx,Z) = kmedoids([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,dist = (x,y) -> norm(x-y)^2,initStrategy=\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The EM algorithm (Lecture/segment 16.5 of https://www.edx.org/course/machine-learning-with-python-from-linear-models-to)\n",
    "\n",
    "\"\"\" log-PDF of a multidimensional normal with no covariance and shared variance across dimensions\"\"\"\n",
    "logNormalFixedSd(x,μ,σ²) = - (length(x)/2) * log(2π*σ²)  -  norm(x-μ)^2/(2σ²)\n",
    "\n",
    "\"\"\" LogSumExp for efficiently computing log(sum(exp.(x))) \"\"\"\n",
    "myLSE(x) = maximum(x)+log(sum(exp.(x .- maximum(x))))\n",
    "\n",
    "\"\"\"Transform an Array{T,1} in an Array{T,2} and leave unchanged Array{T,2}.\"\"\"\n",
    "make_matrix(x::Array) = ndims(x) == 1 ? reshape(x, (size(x)...,1)) : x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "em"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "  em(X,K;p₀,μ₀,σ²₀,tol,msgStep,minVariance,missingValue)\n",
    "\n",
    "Compute Expectation-Maximisation algorithm to identify K clusters of X data assuming a Gaussian Mixture probabilistic Model.\n",
    "\n",
    "X can contain missing values in some or all of its dimensions. In such case the learning is done only with the available data.\n",
    "Implemented in the log-domain for better numerical accuracy with many dimensions.\n",
    "\n",
    "# Parameters:\n",
    "* `X`  :          A (n x d) data to clusterise\n",
    "* `K`  :          Number of cluster wanted\n",
    "* `p₀` :          Initial probabilities of the categorical distribution (K x 1) [default: `nothing`]\n",
    "* `μ₀` :          Initial means (K x d) of the Gaussian [default: `nothing`]\n",
    "* `σ²₀`:          Initial variance of the gaussian (K x 1). We assume here that the gaussian has the same variance across all the dimensions [default: `nothing`]\n",
    "* `tol`:          Tolerance to stop the algorithm [default: 10^(-6)]\n",
    "* `msgStep` :     Iterations between update messages. Use 0 for no updates [default: 10]\n",
    "* `minVariance`:  Minimum variance for the mixtures [default: 0.25]\n",
    "* `missingValue`: Value to be considered as missing in the X [default: `missing`]\n",
    "\n",
    "# Returns:\n",
    "* A named touple of:\n",
    "  * `pⱼₓ`: Matrix of size (N x K) of the probabilities of each point i to belong to cluster j\n",
    "  * `pⱼ` : Probabilities of the categorical distribution (K x 1)\n",
    "  * `μ`  : Means (K x d) of the Gaussian\n",
    "  * `σ²` : Variance of the gaussian (K x 1). We assume here that the gaussian has the same variance across all the dimensions\n",
    "  * `ϵ`  : Vector of the discrepancy (matrix norm) between pⱼₓ and the lagged pⱼₓ at each iteration\n",
    "  * `lL` : The log-likelihood (without considering the last mixture optimisation)\n",
    "  * `BIC` : The Bayesian Information Criterion\n",
    "\n",
    "# Example:\n",
    "```julia\n",
    "julia> clusters = em([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,msgStep=1,missingValue=0)\n",
    "```\n",
    "\"\"\"\n",
    "function em(X,K;p₀=nothing,μ₀=nothing,σ²₀=nothing,tol=10^(-6),msgStep=10,minVariance=0.25,missingValue=missing)\n",
    "    # debug:\n",
    "    #X = [1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4]\n",
    "    #K = 3\n",
    "    #p₀=nothing; μ₀=nothing; σ²₀=nothing; tol=0.0001; msgStep=1; minVariance=0.25; missingValue = 0\n",
    "\n",
    "    X     = make_matrix(X)\n",
    "    (N,D) = size(X)\n",
    "\n",
    "    # Initialisation of the parameters if not provided\n",
    "    minX = fill(-Inf,D)\n",
    "    maxX = fill(Inf,D)\n",
    "    varX_byD = fill(0,D)\n",
    "    for d in 1:D\n",
    "      @inbounds minX[d]  = minimum(skipmissing(X[:,d]))\n",
    "      @inbounds maxX[d]  = maximum(skipmissing(X[:,d]))\n",
    "      varX_byD = max(minVariance, var(skipmissing(X[:,d])))\n",
    "    end\n",
    "    varX = mean(varX_byD)/K^2\n",
    "\n",
    "    pⱼ = isnothing(p₀) ? fill(1/K,K) : p₀\n",
    "    if !isnothing(μ₀)\n",
    "        μ₀  = make_matrix(μ₀)\n",
    "        μ = μ₀\n",
    "    else\n",
    "        μ = zeros(Float64,K,D)\n",
    "        for d in 1:D\n",
    "                μ[:,d] = collect(range(minX[d], stop=maxX[d], length=K))\n",
    "        end\n",
    "    end\n",
    "    σ² = isnothing(σ²₀) ? fill(varX,K) : σ²₀\n",
    "    pⱼₓ = zeros(Float64,N,K) # The posteriors, i.e. the prob that item n belong to cluster k\n",
    "    ϵ = Float64[]\n",
    "\n",
    "    # Checking dimensions only once (but adding then inbounds doesn't change anything. Still good\n",
    "    # to provide a nice informative message)\n",
    "    if size(pⱼ) != (K,) || size(μ) != (K,D) || size(σ²) != (K,)\n",
    "        error(\"Error in the dimensions of the inputs. Please check them.\")\n",
    "    end\n",
    "\n",
    "    # finding empty/non_empty values\n",
    "    XMask = ismissing(missingValue) ?  .! ismissing.(X)  : (X .!= missingValue)\n",
    "    XdimCount = sum(XMask, dims=2)\n",
    "\n",
    "    lL = -Inf\n",
    "    while(true)\n",
    "        oldlL = lL\n",
    "        # E Step: assigning the posterior prob p(j|xi) and computing the log-Likelihood of the parameters given the set of data\n",
    "        # (this last one for informative purposes and terminating the algorithm)\n",
    "        pⱼₓlagged = copy(pⱼₓ)\n",
    "        logpⱼₓ = log.(pⱼₓ)\n",
    "        lL = 0\n",
    "        for n in 1:N\n",
    "            if any(XMask[n,:]) # if at least one true\n",
    "                Xu = X[n,XMask[n,:]]\n",
    "                logpx = myLSE([log(pⱼ[k] + 1e-16) + logNormalFixedSd(Xu,μ[k,XMask[n,:]],σ²[k]) for k in 1:K])\n",
    "                lL += logpx\n",
    "                #px = sum([pⱼ[k]*normalFixedSd(Xu,μ[k,XMask[n,:]],σ²[k]) for k in 1:K])\n",
    "                for k in 1:K\n",
    "                    logpⱼₓ[n,k] = log(pⱼ[k] + 1e-16)+logNormalFixedSd(Xu,μ[k,XMask[n,:]],σ²[k])-logpx\n",
    "                end\n",
    "            else\n",
    "                logpⱼₓ[n,:] = log.(pⱼ)\n",
    "            end\n",
    "        end\n",
    "        pⱼₓ = exp.(logpⱼₓ)\n",
    "\n",
    "        push!(ϵ,norm(pⱼₓlagged - pⱼₓ))\n",
    "\n",
    "        # M step: find parameters that maximise the likelihood\n",
    "        nⱼ = sum(pⱼₓ,dims=1)'\n",
    "        n  = sum(nⱼ)\n",
    "        pⱼ = nⱼ ./ n\n",
    "\n",
    "        #μ  = (pⱼₓ' * X) ./ nⱼ\n",
    "        for d in 1:D\n",
    "            for k in 1:K\n",
    "                nᵢⱼ = sum(pⱼₓ[XMask[:,d],k])\n",
    "                if nᵢⱼ > 1\n",
    "                    μ[k,d] = sum(pⱼₓ[XMask[:,d],k] .* X[XMask[:,d],d])/nᵢⱼ\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #σ² = [sum([pⱼₓ[n,j] * norm(X[n,:]-μ[j,:])^2 for n in 1:N]) for j in 1:K ] ./ (nⱼ .* D)\n",
    "        for k in 1:K\n",
    "            den = dot(XdimCount,pⱼₓ[:,k])\n",
    "            nom = 0.0\n",
    "            for n in 1:N\n",
    "                if any(XMask[n,:])\n",
    "                    nom += pⱼₓ[n,k] * norm(X[n,XMask[n,:]]-μ[k,XMask[n,:]])^2\n",
    "                end\n",
    "            end\n",
    "            if(den> 0 && (nom/den) > minVariance)\n",
    "                σ²[k] = nom/den\n",
    "            else\n",
    "                σ²[k] = minVariance\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Information. Note the likelihood is whitout accounting for the new mu, sigma\n",
    "        if msgStep != 0 && (length(ϵ) % msgStep == 0 || length(ϵ) == 1)\n",
    "            println(\"Iter. $(length(ϵ)):\\tVar. of the post  $(ϵ[end]) \\t  Log-likelihood $(lL)\")\n",
    "        end\n",
    "\n",
    "        # Closing conditions. Note that the logLikelihood is those without considering the new mu,sigma\n",
    "        if (lL - oldlL) <= (tol * abs(lL))\n",
    "            npar = K * D + K + (K-1)\n",
    "            BIC  = lL - (1/2) * npar * log(N)\n",
    "        #if (ϵ[end] < tol)\n",
    "           return (pⱼₓ=pⱼₓ,pⱼ=pⱼ,μ=μ,σ²=σ²,ϵ=ϵ,lL=lL,BIC=BIC)\n",
    "        end\n",
    "    end # end while loop\n",
    "end # end function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter. 1:\tVar. of the post  2.7798403823788407 \t  Log-likelihood -62.16435618142972\n",
      "Iter. 2:\tVar. of the post  0.5606080950482362 \t  Log-likelihood -51.82785452710985\n",
      "Iter. 3:\tVar. of the post  0.3047407377931759 \t  Log-likelihood -47.21642564372429\n",
      "Iter. 4:\tVar. of the post  0.003227835533034755 \t  Log-likelihood -40.26621217189606\n",
      "Iter. 5:\tVar. of the post  5.609004006879284e-16 \t  Log-likelihood -40.26558370139746\n",
      "Iter. 6:\tVar. of the post  1.7801324102294862e-27 \t  Log-likelihood -40.26558370139746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9×3 Array{Float64,2}:\n",
       " 2.90709e-158  1.0          6.00753e-27\n",
       " 1.10092e-161  1.0          2.5648e-26 \n",
       " 4.57484e-102  1.0          2.31584e-31\n",
       " 1.12191e-270  1.0          9.14102e-18\n",
       " 0.0           8.60227e-57  1.0        \n",
       " 0.0           1.7555e-29   1.0        \n",
       " 0.0           1.33625e-49  1.0        \n",
       " 1.0           1.59504e-14  6.0099e-59 \n",
       " 1.0           9.36404e-15  2.97135e-59"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = em([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,msgStep=1)\n",
    "clusters.pⱼₓ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd(@__DIR__)\n",
    "K = [1,12]\n",
    "seeds = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Working with (k,seed) = (1, 0)\n",
      "[INFO] Working with (k,seed) = (1, 1)\n",
      "[INFO] Working with (k,seed) = (1, 2)\n",
      "[INFO] Working with (k,seed) = (1, 3)\n",
      "[INFO] Working with (k,seed) = (1, 4)\n",
      "Upper logLikelihood with 1 clusters: -1307.2234317600933 (seed 0)\n",
      "[INFO] Working with (k,seed) = (12, 0)\n",
      "[INFO] Working with (k,seed) = (12, 1)\n",
      "[INFO] Working with (k,seed) = (12, 2)\n",
      "[INFO] Working with (k,seed) = (12, 3)\n",
      "[INFO] Working with (k,seed) = (12, 4)\n",
      "Upper logLikelihood with 12 clusters: -1118.6190434326675 (seed 2)\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "baseDir = \"assets/netflix/toy_data/\"\n",
    "X = readdlm(joinpath(baseDir,\"toy_data.txt\"))\n",
    "for k in K\n",
    "    ulL = -Inf\n",
    "    bestSeed = -1\n",
    "    bestOut = nothing\n",
    "    for s in seeds\n",
    "        println(\"[INFO] Working with (k,seed) = ($(k), $(s))\")\n",
    "        μ₀ = readdlm(joinpath(baseDir,\"init_mu_$(k)_$(s).csv\"), ' ')\n",
    "        σ²₀ = dropdims(readdlm(joinpath(baseDir,\"init_var_$(k)_$(s).csv\"), ' '),dims=2)\n",
    "        p₀ = dropdims(readdlm(joinpath(baseDir,\"init_p_$(k)_$(s).csv\"), ' '),dims=2)\n",
    "        emOut = em(X,k;p₀=p₀,μ₀=μ₀,σ²₀=σ²₀,msgStep=0,missingValue=0)\n",
    "        lL  = emOut.lL\n",
    "        if lL > ulL\n",
    "            ulL = lL\n",
    "            bestSeed = s\n",
    "            bestOut = emOut\n",
    "        end\n",
    "    end\n",
    "    println(\"Upper logLikelihood with $(k) clusters: $(ulL) (seed $(bestSeed))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Working with (k,seed) = (1, 0)\n",
      "[INFO] Working with (k,seed) = (1, 1)\n",
      "[INFO] Working with (k,seed) = (1, 2)\n",
      "[INFO] Working with (k,seed) = (1, 3)\n",
      "[INFO] Working with (k,seed) = (1, 4)\n",
      "Upper logLikelihood with 1 clusters: -1.5210609539852452e6 (seed 0)\n",
      "[INFO] Working with (k,seed) = (12, 0)\n",
      "[INFO] Working with (k,seed) = (12, 1)\n",
      "[INFO] Working with (k,seed) = (12, 2)\n",
      "[INFO] Working with (k,seed) = (12, 3)\n",
      "[INFO] Working with (k,seed) = (12, 4)\n",
      "Upper logLikelihood with 12 clusters: -1.3902809991574623e6 (seed 1)\n"
     ]
    }
   ],
   "source": [
    "# Full NetFlix dataset\n",
    "baseDir = \"assets/netflix/full/\"\n",
    "X = convert(Array{Int64,2},readdlm(joinpath(baseDir,\"netflix_incomplete.txt\")))\n",
    "for k in K\n",
    "    ulL = -Inf\n",
    "    bestSeed = -1\n",
    "    bestOut = nothing\n",
    "    for s in seeds\n",
    "        println(\"[INFO] Working with (k,seed) = ($(k), $(s))\")\n",
    "        μ₀  = readdlm(joinpath(baseDir,\"init_mu_$(k)_$(s).csv\"), ' ')\n",
    "        σ²₀ = dropdims(readdlm(joinpath(baseDir,\"init_var_$(k)_$(s).csv\"), ' '),dims=2)\n",
    "        p₀  = dropdims(readdlm(joinpath(baseDir,\"init_p_$(k)_$(s).csv\"), ' '),dims=2)\n",
    "        emOut = em(X,k;p₀=p₀,μ₀=μ₀,σ²₀=σ²₀,msgStep=0,missingValue=0)\n",
    "        lL  = emOut.lL\n",
    "        if lL > ulL\n",
    "            ulL = lL\n",
    "            bestSeed = s\n",
    "            bestOut = emOut\n",
    "        end\n",
    "    end\n",
    "    println(\"Upper logLikelihood with $(k) clusters: $(ulL) (seed $(bestSeed))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collFilteringGMM"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "  collFilteringGMM(X,K;p₀,μ₀,σ²₀,tol,msgStep,minVariance,missingValue)\n",
    "\n",
    "Fill missing entries in a sparse matrix assuming an underlying Gaussian Mixture probabilistic Model (GMM) and implementing\n",
    "an Expectation-Maximisation algorithm.\n",
    "\n",
    "Implemented in the log-domain for better numerical accuracy with many dimensions.\n",
    "\n",
    "# Parameters:\n",
    "* `X`  :          A (N x D) sparse matrix of data to fill according to a GMM model\n",
    "* `K`  :          Number of mixtures desired\n",
    "* `p₀` :          Initial probabilities of the categorical distribution (K x 1) [default: `nothing`]\n",
    "* `μ₀` :          Initial means (K x D) of the Gaussian [default: `nothing`]\n",
    "* `σ²₀`:          Initial variance of the gaussian (K x 1). We assume here that the gaussian has the same variance across all the dimensions [default: `nothing`]\n",
    "* `tol`:          Tolerance to stop the algorithm [default: 10^(-6)]\n",
    "* `msgStep` :     Iterations between update messages. Use 0 for no updates [default: 10]\n",
    "* `minVariance`:  Minimum variance for the mixtures [default: 0.25]\n",
    "* `missingValue`: Value to be considered as missing in the X [default: `missing`]\n",
    "\n",
    "# Returns:\n",
    "* A named touple of:\n",
    "  * `̂X̂`    : The Filled Matrix of size (N x D)\n",
    "  * `nFill`: The number of items filled\n",
    "  * `lL`   : The log-likelihood (without considering the last mixture optimisation)\n",
    "  * `BIC`  : The Bayesian Information Criterion\n",
    "\n",
    "# Example:\n",
    "```julia\n",
    "julia>  cFOut = collFilteringGMM([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,msgStep=1,missingValue=0)\n",
    "```\n",
    "\"\"\"\n",
    "function collFilteringGMM(X,K;p₀=nothing,μ₀=nothing,σ²₀=nothing,tol=10^(-6),msgStep=10,minVariance=0.25,missingValue=missing)\n",
    "    emOut = em(X,K;p₀=p₀,μ₀=μ₀,σ²₀=σ²₀,tol=tol,msgStep=msgStep,minVariance=minVariance,missingValue=missingValue)\n",
    "    (N,D) = size(X)\n",
    "    XMask = ismissing(missingValue) ?  .! ismissing.(X)  : (X .!= missingValue)\n",
    "    nFill = (N * D) - sum(XMask)\n",
    "    X̂ = copy(X)\n",
    "\n",
    "    for n in 1:N\n",
    "        for d in 1:D\n",
    "            if !XMask[n,d]\n",
    "                 X̂[n,d] = dot(emOut.μ[:,d],emOut.pⱼₓ[n,:])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return (X̂=X̂,nFill=nFill,lL=emOut.lL,BIC=emOut.BIC)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×2 Array{Float64,2}:\n",
       " 1.0  10.5\n",
       " 1.5   0.0\n",
       " 1.8   8.0\n",
       " 1.7  15.0\n",
       " 3.2  40.0\n",
       " 0.0   0.0\n",
       " 3.3  38.0\n",
       " 0.0  -2.3\n",
       " 5.2  -2.4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter. 1:\tVar. of the post  2.61937747932065 \t  Log-likelihood -47.59140596017498\n",
      "Iter. 2:\tVar. of the post  0.5226030386857065 \t  Log-likelihood -34.55184066668723\n",
      "Iter. 3:\tVar. of the post  0.3500981768393402 \t  Log-likelihood -32.92185047653772\n",
      "Iter. 4:\tVar. of the post  0.32940171779360017 \t  Log-likelihood -30.01085600946215\n",
      "Iter. 5:\tVar. of the post  0.05092179105118827 \t  Log-likelihood -27.686896657600293\n",
      "Iter. 6:\tVar. of the post  0.01144416282455234 \t  Log-likelihood -27.681990100476558\n",
      "Iter. 7:\tVar. of the post  0.004605091358874689 \t  Log-likelihood -27.681832719530703\n",
      "Iter. 8:\tVar. of the post  0.0022110716618263934 \t  Log-likelihood -27.68179603140188\n",
      "Iter. 9:\tVar. of the post  0.0010765120575048945 \t  Log-likelihood -27.68178722759999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9×2 Array{Float64,2}:\n",
       " 1.0     10.5   \n",
       " 1.5     14.1779\n",
       " 1.8      8.0   \n",
       " 1.7     15.0   \n",
       " 3.2     40.0   \n",
       " 2.8627  15.1255\n",
       " 3.3     38.0   \n",
       " 5.2     -2.3   \n",
       " 5.2     -2.4   "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cFOut = collFilteringGMM(X,3,msgStep=1,missingValue=0)\n",
    "cFOut.X̂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
