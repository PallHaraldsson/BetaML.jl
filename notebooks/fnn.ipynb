{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Neural Network Class\n",
    "# ==================================\n",
    "\n",
    "using Random\n",
    "\n",
    "## Some utility functions..\n",
    "import Base.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeRowVector (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "reshape(myNumber, dims..) - Reshape a number as a n dimensional Array\n",
    "\"\"\"\n",
    "function reshape(x::T, dims...) where {T <: Number}\n",
    "   x = [x]\n",
    "   reshape(x,dims)\n",
    "end\n",
    "function makeColVector(x::T) where {T <: Number}\n",
    "    return [x]\n",
    "end\n",
    "function makeColVector(x::T) where {T <: AbstractArray}\n",
    "    reshape(x,length(x))\n",
    "end\n",
    "function makeRowVector(x::T) where {T <: Number}\n",
    "    return [x]'\n",
    "end\n",
    "function makeRowVector(x::T) where {T <: AbstractArray}\n",
    "    reshape(x,1,length(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   Layer\n",
    "\n",
    "Representation of a layer in the network\n",
    "\n",
    "# Fields:\n",
    "* `w`:  Weigths matrix with respect to the input from previous layer or data (n pr. layer x n)\n",
    "* `wb`: Biases (n)\n",
    "* `f`:  Activation function\n",
    "* `df`: Derivative of the activation function\n",
    "\"\"\"\n",
    "mutable struct Layer\n",
    "     w::Array{Float64,2}\n",
    "     wb::Array{Float64,1}\n",
    "     f::Function\n",
    "     df::Function\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   FNN\n",
    "\n",
    "Representation of a Forward Neural Network\n",
    "\n",
    "# Fields:\n",
    "* `layers`:  Array of layers objects\n",
    "* `cf`:      Cost function\n",
    "* `dcf`:     Derivative of the cost function\n",
    "* `trained`: Control flag for trained networks\n",
    "\"\"\"\n",
    "mutable struct FNN\n",
    "    layers::Array{Layer,1}\n",
    "    cf::Function \n",
    "    dcf::Function\n",
    "    trained::Bool\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildLayer"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   buildLayer(f,df,n,nₗ;w,wb)\n",
    "\n",
    "Instantiate a new layer\n",
    "\n",
    "Parameters:\n",
    "* `f`:  Activation function\n",
    "* `df`: Derivative of the activation function\n",
    "* `n`:  Number of nodes\n",
    "* `nₗ`: Number of nodes of the previous layer\n",
    "* `w`:  Initial weigths with respect to input [default: `rand(nₗ,n)`]\n",
    "* `wb`: Initial weigths with respect to bias [default: `rand(n)`]\n",
    "\n",
    "\"\"\"\n",
    "function buildLayer(f,df,n,nₗ;w=rand(nₗ,n),wb=rand(n))\n",
    "    # To be sure w is a matrix and wb a column vector..\n",
    "    w  = reshape(w,nₗ,n)\n",
    "    wb = reshape(wb,n)\n",
    "    return Layer(w,wb,f,df)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildNetwork"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   buildNetwork\n",
    "\n",
    "Instantiate a new Feedforward Neural Network\n",
    "\n",
    "Parameters:\n",
    "* `layers`:  Array of layers objects\n",
    "* `cf`:      Cost function\n",
    "* `dcf`:     Derivative of the cost function\n",
    "\n",
    "# Notes:\n",
    "* Even if the network ends with a single output note, the cost function and its\n",
    "derivative should always expect y and ŷ as column vectors.\n",
    "\"\"\"\n",
    "function buildNetwork(layers,cf,dcf)\n",
    "    return FNN(layers,cf,dcf,false)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   predict(layer,x)\n",
    "\n",
    "Layer prediction of a single data point\n",
    "\n",
    "# Parameters:\n",
    "* `layer`:  Worker layer\n",
    "* `x`:      Input to the layer\n",
    "\"\"\"\n",
    "function predict(layer::Layer,x)\n",
    "  return layer.f.((reshape(x,1,length(x))*layer.w)' + layer.wb)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   predict(fnn,x)\n",
    "\n",
    "Network prediction of a single data point\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`:  Worker network\n",
    "* `x`:    Input to the network\n",
    "\"\"\"\n",
    "function predict(fnn::FNN,x)\n",
    "    makeColVector(x)\n",
    "    values = x\n",
    "    for l in fnn.layers\n",
    "        values = predict(l,values)\n",
    "    end\n",
    "    return values\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   error(fnn,x,y)\n",
    "\n",
    "Compute network loss on a single data point\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "* `x`:   Input to the network\n",
    "* `y`:   Label input\n",
    "\"\"\"\n",
    "function error(fnn::FNN,x,y)\n",
    "    x = makeColVector(x)\n",
    "    y = makeColVector(y)\n",
    "    ŷ = predict(fnn,x)\n",
    "    return fnn.cf(ŷ,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "errors"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   errors(fnn,x,y)\n",
    "\n",
    "Compute avg. network loss on a test set\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "* `x`:   Input to the network (n x d)\n",
    "* `y`:   Label input (n) or (n x d)\n",
    "\"\"\"\n",
    "function errors(fnn::FNN,x,y)\n",
    "    fnn.trained ? \"\" : @warn \"Seems you are trying to test a neural network that has not been tested. Use first `test!(rnn,x,y)`\"\n",
    "    ϵ = 0\n",
    "    for i in 1:size(x)[1]\n",
    "        xᵢ = x[i,:]'\n",
    "        yᵢ = y[i,:]'\n",
    "        ϵ += error(fnn,xᵢ,yᵢ)\n",
    "    end\n",
    "    return ϵ/size(x)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getW"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   getW(fnn)\n",
    "\n",
    "Retrieve current weigthts\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "\n",
    "# Notes:\n",
    "* The output is a vector of tuples of each layer's input weigths and bias weigths\n",
    "\"\"\"\n",
    "function getW(fnn)\n",
    "  w = Tuple{Array{Float64,2},Array{Float64,1}}[]\n",
    "  for l in fnn.layers\n",
    "      push!(w,(l.w,l.wb))\n",
    "  end\n",
    "  return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getDW"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   getDW(fnn,x,y)\n",
    "\n",
    "Retrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "* `x`:   Input to the network\n",
    "* `y`:   Label input\n",
    "\n",
    "#Notes:\n",
    "* The output is a vector of tuples of each layer's input weigths and bias weigths\n",
    "\"\"\"\n",
    "function getDW(fnn,x,y)\n",
    "  x = makeColVector(x)\n",
    "  y = makeColVector(y)\n",
    "  lz = Array{Float64,1}[]\n",
    "  lo = Array{Float64,1}[]\n",
    "  dW = Tuple{Array{Float64,2},Array{Float64,1}}[]\n",
    "\n",
    "  push!(lz,x)\n",
    "  push!(lo,x)\n",
    "\n",
    "  for l in fnn.layers\n",
    "      x = lo[end]\n",
    "      z = dropdims((reshape(x,1,length(x))*l.w)' + l.wb,dims=2)\n",
    "      o = l.f.(z)\n",
    "      push!(lz, z)\n",
    "      push!(lo, o)\n",
    "  end\n",
    "  dc = fnn.dcf(lo[end],y)\n",
    "  δ = dc # derivative of the cost function with respect to the layer output\n",
    "\n",
    "  # backpropagation step\n",
    "  for lidx in length(fnn.layers):-1:1\n",
    "     l = fnn.layers[lidx]\n",
    "     # Note that lz and lo vectors includes x, so the second layer is the third element in the vector\n",
    "     dwb = l.df.(lz[lidx+1]) .* δ # derivative with respect to the layer biases\n",
    "     dw = lo[lidx] * dwb'         # derivative with respect to the layer input weigths\n",
    "     push!(dW,(dw,dwb))\n",
    "     # Computing derivatives of the cost function with respect of the output of the previous layer\n",
    "     δ = l.w * dwb\n",
    "  end\n",
    "  return dW[end:-1:1] # reversing it, to start from the first layer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateWeights!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   updateWeights!(fnn,w)\n",
    "\n",
    "Update weigths of the network\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "* `w`:   The new weights to set\n",
    "\"\"\"\n",
    "function updateWeights!(fnn,w)\n",
    "    for lidx in 1:length(fnn.layers)\n",
    "        fnn.layers[lidx].w = w[lidx][1]\n",
    "        fnn.layers[lidx].wb = w[lidx][2]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train!"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   train!(fnn,x,y;epochs,η,rshuffle)\n",
    "\n",
    "Train a fnn with the given x,y data\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`:      Worker network\n",
    "* `x`:        Training input to the network (records x dimensions)\n",
    "* `y`:        Label input (records)\n",
    "* `epochs`:   Number of passages over the training set [def = `1000`]\n",
    "* `η`:        Learning rate. If not provided 1/(1+epoch) is used [def = `nothing`]\n",
    "* `rshuffle`: Whether to random shuffle the training set at each epoch [def = `true`]\n",
    "\"\"\"\n",
    "function train!(fnn,x,y;epochs=1000, η=nothing, rshuffle=true)\n",
    "    logStep = Int64(ceil(epochs/100))\n",
    "    dyn_η = η == nothing ? true : false\n",
    "    for t in 1:epochs\n",
    "        if rshuffle\n",
    "           # random shuffle x and y\n",
    "           ridx = shuffle(1:size(x)[1])\n",
    "           x = x[ridx, :]\n",
    "           y = y[ridx , :]\n",
    "        end\n",
    "        ϵ = 0\n",
    "        η = dyn_η ? 1/(1+t) : η\n",
    "        for i in 1:size(x)[1]\n",
    "            xᵢ = x[i,:]'\n",
    "            yᵢ = makeColVector(y[i])\n",
    "            w  = getW(fnn)\n",
    "            dW = getDW(fnn,xᵢ,yᵢ)\n",
    "            for (lidx,l) in enumerate(fnn.layers)\n",
    "                l.w  = l.w -  η .* dW[lidx][1]\n",
    "                l.wb = l.wb - η .* dW[lidx][2]\n",
    "            end\n",
    "            ϵ += error(fnn,xᵢ,yᵢ)\n",
    "        end\n",
    "        (t % logStep == 0) || t == 1 || t == epochs ? println(\"Avg. error after epoch $t : $(ϵ/size(x)[1])\") : \"\"\n",
    "    end\n",
    "    fnn.trained = true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN(Layer[Layer([1.0 1.0 1.0; 1.0 1.0 1.0], [0.0, 0.0, 0.0], relu, drelu), Layer([1.0; 1.0; 1.0], [0.0], linearf, dlinearf)], cost, dcost, false)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================\n",
    "# Specific implementation - FNN definition\n",
    "# ==================================\n",
    "\n",
    "# Defining the functions we fill use as activation function as well their derivatives\n",
    "# (yes, we could have used instead an automatic differentiation - AD - library..)\n",
    "relu(x)     = max(0,x)\n",
    "drelu(x)    = x <= 0 ? 0 : 1\n",
    "linearf(x)  = x\n",
    "dlinearf(x) = 1\n",
    "cost(ŷ,y)   = (1/2)*(y[1]-ŷ[1])^2\n",
    "dcost(ŷ,y)  = [- (y[1]-ŷ[1])]\n",
    "\n",
    "l1 = buildLayer(relu,drelu,3,2,w=[1 1 1;1 1 1],wb=[0,0,0])\n",
    "l2 = buildLayer(linearf,dlinearf,1,3,w=[1,1,1],wb=0)\n",
    "myfnn = buildNetwork([l1,l2],cost,dcost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. error after epoch 1 : 3.6532211152267577\n",
      "Avg. error after epoch 2 : 2.0839147371036453\n",
      "Avg. error after epoch 3 : 1.9965460916812372\n",
      "Avg. error after epoch 4 : 1.9718232202297679\n",
      "Avg. error after epoch 5 : 1.952221707019908\n",
      "Avg. error after epoch 6 : 1.9333863621103424\n",
      "Avg. error after epoch 7 : 1.9148178098408046\n",
      "Avg. error after epoch 8 : 1.8964517283157738\n",
      "Avg. error after epoch 9 : 1.878277578397346\n",
      "Avg. error after epoch 10 : 1.8602916021318865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10826685330049687"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================\n",
    "# Usage of the FNN\n",
    "# ==================================\n",
    "\n",
    "xtrain = [2 1; 3 3; 4 5; 6 6]\n",
    "ytrain = [10,21,32,42]\n",
    "ytrain = [14,21,28,42]\n",
    "xtest  = [1 1; 2 2; 3 3; 5 5; 10 10]\n",
    "ytest  = [7,14,21,35,70]\n",
    "\n",
    "train!(myfnn,xtrain,ytrain,epochs=10,η=0.001,rshuffle=false) # 1.86\n",
    "errors(myfnn,xtest,ytest) # 0.108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. error after epoch 1 : 366.01964165426426\n",
      "Avg. error after epoch 100 : 63.769571965511105\n",
      "Avg. error after epoch 200 : 26.572106812681255\n",
      "Avg. error after epoch 300 : 15.711577712709587\n",
      "Avg. error after epoch 400 : 10.548392446749844\n",
      "Avg. error after epoch 500 : 5.985948012098555\n",
      "Avg. error after epoch 600 : 10.940899264325697\n",
      "Avg. error after epoch 700 : 3.460469363730156\n",
      "Avg. error after epoch 800 : 1.0537976130717779\n",
      "Avg. error after epoch 900 : 0.5161456876864611\n",
      "Avg. error after epoch 1000 : 0.19090969358769672\n",
      "Avg. error after epoch 1100 : 0.13821619552769834\n",
      "Avg. error after epoch 1200 : 0.11617293311077119\n",
      "Avg. error after epoch 1300 : 2.136130570090624\n",
      "Avg. error after epoch 1400 : 4.4632049140144465\n",
      "Avg. error after epoch 1500 : 2.8362838096140517\n",
      "Avg. error after epoch 1600 : 1.520739967965049\n",
      "Avg. error after epoch 1700 : 1.1521066045730337\n",
      "Avg. error after epoch 1800 : 0.9521991705744707\n",
      "Avg. error after epoch 1900 : 0.8237972538559106\n",
      "Avg. error after epoch 2000 : 0.7321623956601785\n",
      "Avg. error after epoch 2100 : 0.6619272570764321\n",
      "Avg. error after epoch 2200 : 0.6053741075355884\n",
      "Avg. error after epoch 2300 : 0.5582494159220814\n",
      "Avg. error after epoch 2400 : 0.5180235136871002\n",
      "Avg. error after epoch 2500 : 0.48308622778141874\n",
      "Avg. error after epoch 2600 : 0.4523465826815305\n",
      "Avg. error after epoch 2700 : 0.42502298271792105\n",
      "Avg. error after epoch 2800 : 0.400528828735439\n",
      "Avg. error after epoch 2900 : 0.3784079271277024\n",
      "Avg. error after epoch 3000 : 0.3582966002190101\n",
      "Avg. error after epoch 3100 : 0.3399004026536899\n",
      "Avg. error after epoch 3200 : 0.3229789367769431\n",
      "Avg. error after epoch 3300 : 0.3073351914568219\n",
      "Avg. error after epoch 3400 : 0.2928074377176816\n",
      "Avg. error after epoch 3500 : 0.2792626504573572\n",
      "Avg. error after epoch 3600 : 0.2665909796654422\n",
      "Avg. error after epoch 3700 : 0.2547010928583649\n",
      "Avg. error after epoch 3800 : 0.2435163292445541\n",
      "Avg. error after epoch 3900 : 0.23297161723745324\n",
      "Avg. error after epoch 4000 : 0.22301107532623743\n",
      "Avg. error after epoch 4100 : 0.21358618585601002\n",
      "Avg. error after epoch 4200 : 0.20465442030459208\n",
      "Avg. error after epoch 4300 : 0.19617820304131994\n",
      "Avg. error after epoch 4400 : 0.1881241202563861\n",
      "Avg. error after epoch 4500 : 0.18046230345680456\n",
      "Avg. error after epoch 4600 : 0.17316593731862662\n",
      "Avg. error after epoch 4700 : 0.16621085758940352\n",
      "Avg. error after epoch 4800 : 0.15957521604496894\n",
      "Avg. error after epoch 4900 : 0.1532391970620872\n",
      "Avg. error after epoch 5000 : 0.14718477522694445\n",
      "Avg. error after epoch 5100 : 0.14139550646830287\n",
      "Avg. error after epoch 5200 : 0.13585634714985773\n",
      "Avg. error after epoch 5300 : 0.1305534968209932\n",
      "Avg. error after epoch 5400 : 0.12547426118036104\n",
      "Avg. error after epoch 5500 : 0.12060693241723956\n",
      "Avg. error after epoch 5600 : 0.1159406845531365\n",
      "Avg. error after epoch 5700 : 0.11146548176546389\n",
      "Avg. error after epoch 5800 : 0.10717199796686355\n",
      "Avg. error after epoch 5900 : 0.10305154615587933\n",
      "Avg. error after epoch 6000 : 0.09909601625953672\n",
      "Avg. error after epoch 6100 : 0.0952978203626714\n",
      "Avg. error after epoch 6200 : 0.09164984436820288\n",
      "Avg. error after epoch 6300 : 0.088145405261503\n",
      "Avg. error after epoch 6400 : 0.08477821326212343\n",
      "Avg. error after epoch 6500 : 0.08154233824236831\n",
      "Avg. error after epoch 6600 : 0.07843217987451855\n",
      "Avg. error after epoch 6700 : 0.07544244103960693\n",
      "Avg. error after epoch 6800 : 0.07256810409254726\n",
      "Avg. error after epoch 6900 : 0.0698044096316748\n",
      "Avg. error after epoch 7000 : 0.0671468374664413\n",
      "Avg. error after epoch 7100 : 0.06459108951750286\n",
      "Avg. error after epoch 7200 : 0.06213307441720267\n",
      "Avg. error after epoch 7300 : 0.05976889360874531\n",
      "Avg. error after epoch 7400 : 0.0574948287682658\n",
      "Avg. error after epoch 7500 : 0.055307330395880254\n",
      "Avg. error after epoch 7600 : 0.053203007442148155\n",
      "Avg. error after epoch 7700 : 0.05117861785203361\n",
      "Avg. error after epoch 7800 : 0.049231059924260266\n",
      "Avg. error after epoch 7900 : 0.0473573643960163\n",
      "Avg. error after epoch 8000 : 0.04555468717419582\n",
      "Avg. error after epoch 8100 : 0.04382030264394399\n",
      "Avg. error after epoch 8200 : 0.04215159749370856\n",
      "Avg. error after epoch 8300 : 0.04054606500343195\n",
      "Avg. error after epoch 8400 : 0.039001299748698\n",
      "Avg. error after epoch 8500 : 0.037514992679276986\n",
      "Avg. error after epoch 8600 : 0.03608492653540197\n",
      "Avg. error after epoch 8700 : 0.03470897156944962\n",
      "Avg. error after epoch 8800 : 0.03338508154418948\n",
      "Avg. error after epoch 8900 : 0.032111289982288814\n",
      "Avg. error after epoch 9000 : 0.03088570664455677\n",
      "Avg. error after epoch 9100 : 0.029706514216650694\n",
      "Avg. error after epoch 9200 : 0.02857196518661582\n",
      "Avg. error after epoch 9300 : 0.02748037889731772\n",
      "Avg. error after epoch 9400 : 0.0264301387593147\n",
      "Avg. error after epoch 9500 : 0.025419689611922276\n",
      "Avg. error after epoch 9600 : 0.024447535220635395\n",
      "Avg. error after epoch 9700 : 0.02351223590093788\n",
      "Avg. error after epoch 9800 : 0.022612406259256396\n",
      "Avg. error after epoch 9900 : 0.021746713042814596\n",
      "Avg. error after epoch 10000 : 0.02091387309087929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.6597581080045"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtanh(x)    = 1-tanh(x)^2\n",
    "l1 = buildLayer(tanh,dtanh,3,2)\n",
    "l2 = buildLayer(linearf,dlinearf,1,3)\n",
    "myfnn2 = buildNetwork([l1,l2],cost,dcost)\n",
    "\n",
    "train!(myfnn2,xtrain,ytrain,epochs=10000,η=0.001,rshuffle=false) # 0.011\n",
    "errors(myfnn2,xtest,ytest) # 76.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
