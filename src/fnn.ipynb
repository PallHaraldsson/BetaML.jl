{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train!"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================\n",
    "# Neural Network Class\n",
    "# ==================================\n",
    "\n",
    "using Random\n",
    "\n",
    "## Some utility functions..\n",
    "import Base.reshape\n",
    "\"\"\"\n",
    "reshape(myNumber, dims..) - Reshape a number as a n dimensional Array\n",
    "\"\"\"\n",
    "function reshape(x::T, dims...) where {T <: Number}\n",
    "   x = [x]\n",
    "   reshape(x,dims)\n",
    "end\n",
    "function makeColVector(x::T) where {T <: Number}\n",
    "    return [x]\n",
    "end\n",
    "function makeColVector(x::T) where {T <: AbstractArray}\n",
    "    reshape(x,length(x))\n",
    "end\n",
    "function makeRowVector(x::T) where {T <: Number}\n",
    "    return [x]'\n",
    "end\n",
    "function makeRowVector(x::T) where {T <: AbstractArray}\n",
    "    reshape(x,1,length(x))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "   Layer\n",
    "\n",
    "Representation of a layer in the network\n",
    "\n",
    "# Fields:\n",
    "* `w`:  Weigths matrix with respect to the input from previous layer or data (n pr. layer x n)\n",
    "* `wb`: Biases (n)\n",
    "* `f`:  Activation function\n",
    "* `df`: Derivative of the activation function\n",
    "\"\"\"\n",
    "mutable struct Layer\n",
    "     w::Array{Float64,2}\n",
    "     wb::Array{Float64,1}\n",
    "     f::Function\n",
    "     df::Function\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   FNN\n",
    "\n",
    "Representation of a Forward Neural Network\n",
    "\n",
    "# Fields:\n",
    "* `layers`:  Array of layers objects\n",
    "* `cf`:      Cost function\n",
    "* `dcf`:     Derivative of the cost function\n",
    "* `trained`: Control flag for trained networks\n",
    "\"\"\"\n",
    "mutable struct FNN\n",
    "    layers::Array{Layer,1}\n",
    "    cf::Function \n",
    "    dcf::Function\n",
    "    trained::Bool\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   buildLayer(f,df,n,nₗ;w,wb)\n",
    "\n",
    "Instantiate a new layer\n",
    "\n",
    "Parameters:\n",
    "* `f`:  Activation function\n",
    "* `df`: Derivative of the activation function\n",
    "* `n`:  Number of nodes\n",
    "* `nₗ`: Number of nodes of the previous layer\n",
    "* `w`:  Initial weigths with respect to input [default: `rand(nₗ,n)`]\n",
    "* `wb`: Initial weigths with respect to bias [default: `rand(n)`]\n",
    "\n",
    "\"\"\"\n",
    "function buildLayer(f,df,n,nₗ;w=rand(nₗ,n),wb=rand(n))\n",
    "    # To be sure w is a matrix and wb a column vector..\n",
    "    w  = reshape(w,nₗ,n)\n",
    "    wb = reshape(wb,n)\n",
    "    return Layer(w,wb,f,df)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   buildNetwork\n",
    "\n",
    "Instantiate a new Feedforward Neural Network\n",
    "\n",
    "Parameters:\n",
    "* `layers`:  Array of layers objects\n",
    "* `cf`:      Cost function\n",
    "* `dcf`:     Derivative of the cost function\n",
    "\n",
    "# Notes:\n",
    "* Even if the network ends with a single output note, the cost function and its\n",
    "derivative should always expect y and ŷ as column vectors.\n",
    "\"\"\"\n",
    "function buildNetwork(layers,cf,dcf)\n",
    "    return FNN(layers,cf,dcf,false)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   predict(layer,x)\n",
    "\n",
    "Layer prediction of a single data point\n",
    "\n",
    "# Parameters:\n",
    "* `layer`:  Worker layer\n",
    "* `x`:      Input to the layer\n",
    "\"\"\"\n",
    "function predict(layer::Layer,x)\n",
    "  return layer.f.((reshape(x,1,length(x))*layer.w)' + layer.wb)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   predict(fnn,x)\n",
    "\n",
    "Network prediction of a single data point\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`:  Worker network\n",
    "* `x`:    Input to the network\n",
    "\"\"\"\n",
    "function predict(fnn::FNN,x)\n",
    "    makeColVector(x)\n",
    "    values = x\n",
    "    for l in fnn.layers\n",
    "        values = predict(l,values)\n",
    "    end\n",
    "    return values\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   error(fnn,x,y)\n",
    "\n",
    "Compute network loss on a single data point\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "* `x`:   Input to the network\n",
    "* `y`:   Label input\n",
    "\"\"\"\n",
    "function error(fnn::FNN,x,y)\n",
    "    x = makeColVector(x)\n",
    "    y = makeColVector(y)\n",
    "    ŷ = predict(fnn,x)\n",
    "    return fnn.cf(ŷ,y)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   errors(fnn,x,y)\n",
    "\n",
    "Compute avg. network loss on a test set\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "* `x`:   Input to the network (n x d)\n",
    "* `y`:   Label input (n) or (n x d)\n",
    "\"\"\"\n",
    "function errors(fnn::FNN,x,y)\n",
    "    fnn.trained ? \"\" : @warn \"Seems you are trying to test a neural network that has not been tested. Use first `test!(rnn,x,y)`\"\n",
    "    ϵ = 0\n",
    "    for i in 1:size(x)[1]\n",
    "        xᵢ = x[i,:]'\n",
    "        yᵢ = y[i,:]'\n",
    "        ϵ += error(fnn,xᵢ,yᵢ)\n",
    "    end\n",
    "    return ϵ/size(x)[1]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   getW(fnn)\n",
    "\n",
    "Retrieve current weigthts\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "\n",
    "# Notes:\n",
    "* The output is a vector of tuples of each layer's input weigths and bias weigths\n",
    "\"\"\"\n",
    "function getW(fnn)\n",
    "  w = Tuple{Array{Float64,2},Array{Float64,1}}[]\n",
    "  for l in fnn.layers\n",
    "      push!(w,(l.w,l.wb))\n",
    "  end\n",
    "  return w\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   getDW(fnn,x,y)\n",
    "\n",
    "Retrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "* `x`:   Input to the network\n",
    "* `y`:   Label input\n",
    "\n",
    "#Notes:\n",
    "* The output is a vector of tuples of each layer's input weigths and bias weigths\n",
    "\"\"\"\n",
    "function getDW(fnn,x,y)\n",
    "  x = makeColVector(x)\n",
    "  y = makeColVector(y)\n",
    "  lz = Array{Float64,1}[]\n",
    "  lo = Array{Float64,1}[]\n",
    "  dW = Tuple{Array{Float64,2},Array{Float64,1}}[]\n",
    "\n",
    "  push!(lz,x)\n",
    "  push!(lo,x)\n",
    "\n",
    "  for l in fnn.layers\n",
    "      x = lo[end]\n",
    "      z = dropdims((reshape(x,1,length(x))*l.w)' + l.wb,dims=2)\n",
    "      o = l.f.(z)\n",
    "      push!(lz, z)\n",
    "      push!(lo, o)\n",
    "  end\n",
    "  dc = fnn.dcf(lo[end],y)\n",
    "  δ = dc # derivative of the cost function with respect to the layer output\n",
    "\n",
    "  # backpropagation step\n",
    "  for lidx in length(fnn.layers):-1:1\n",
    "     l = fnn.layers[lidx]\n",
    "     # Note that lz and lo vectors includes x, so the second layer is the third element in the vector\n",
    "     dwb = l.df.(lz[lidx+1]) .* δ # derivative with respect to the layer biases\n",
    "     dw = lo[lidx] * dwb'         # derivative with respect to the layer input weigths\n",
    "     push!(dW,(dw,dwb))\n",
    "     # Computing derivatives of the cost function with respect of the output of the previous layer\n",
    "     δ = l.w * dwb\n",
    "  end\n",
    "  return dW[end:-1:1] # reversing it, to start from the first layer\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   updateWeights!(fnn,w)\n",
    "\n",
    "Update weigths of the network\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`: Worker network\n",
    "* `w`:   The new weights to set\n",
    "\"\"\"\n",
    "function updateWeights!(fnn,w)\n",
    "    for lidx in 1:length(fnn.layers)\n",
    "        fnn.layers[lidx].w = w[lidx][1]\n",
    "        fnn.layers[lidx].wb = w[lidx][2]\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "   train!(fnn,x,y;epochs,η,rshuffle)\n",
    "\n",
    "Train a fnn with the given x,y data\n",
    "\n",
    "# Parameters:\n",
    "* `fnn`:      Worker network\n",
    "* `x`:        Training input to the network (records x dimensions)\n",
    "* `y`:        Label input (records)\n",
    "* `epochs`:   Number of passages over the training set [def = `1000`]\n",
    "* `η`:        Learning rate. If not provided 1/(1+epoch) is used [def = `nothing`]\n",
    "* `rshuffle`: Whether to random shuffle the training set at each epoch [def = `true`]\n",
    "\"\"\"\n",
    "function train!(fnn,x,y;epochs=1000, η=nothing, rshuffle=true)\n",
    "    logStep = Int64(ceil(epochs/100))\n",
    "    dyn_η = η == nothing ? true : false\n",
    "    for t in 1:epochs\n",
    "        if rshuffle\n",
    "           # random shuffle x and y\n",
    "           ridx = shuffle(1:size(x)[1])\n",
    "           x = x[ridx, :]\n",
    "           y = y[ridx , :]\n",
    "        end\n",
    "        ϵ = 0\n",
    "        η = dyn_η ? 1/(1+t) : η\n",
    "        for i in 1:size(x)[1]\n",
    "            xᵢ = x[i,:]'\n",
    "            yᵢ = makeColVector(y[i])\n",
    "            w  = getW(fnn)\n",
    "            dW = getDW(fnn,xᵢ,yᵢ)\n",
    "            for (lidx,l) in enumerate(fnn.layers)\n",
    "                l.w  = l.w -  η .* dW[lidx][1]\n",
    "                l.wb = l.wb - η .* dW[lidx][2]\n",
    "            end\n",
    "            ϵ += error(fnn,xᵢ,yᵢ)\n",
    "        end\n",
    "        (t % logStep == 0) || t == 1 || t == epochs ? println(\"Avg. error after epoch $t : $(ϵ/size(x)[1])\") : \"\"\n",
    "    end\n",
    "    fnn.trained = true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN(Layer[Layer([1.0 1.0 1.0; 1.0 1.0 1.0], [0.0, 0.0, 0.0], relu, drelu), Layer([1.0; 1.0; 1.0], [0.0], linearf, dlinearf)], cost, dcost, false)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================\n",
    "# Specific implementation - FNN definition\n",
    "# ==================================\n",
    "\n",
    "# Defining the functions we fill use as activation function as well their derivatives\n",
    "# (yes, we could have used instead an automatic differentiation - AD - library..)\n",
    "relu(x)     = max(0,x)\n",
    "drelu(x)    = x <= 0 ? 0 : 1\n",
    "linearf(x)  = x\n",
    "dlinearf(x) = 1\n",
    "cost(ŷ,y)   = (1/2)*(y[1]-ŷ[1])^2\n",
    "dcost(ŷ,y)  = [- (y[1]-ŷ[1])]\n",
    "\n",
    "l1 = buildLayer(relu,drelu,3,2,w=[1 1 1;1 1 1],wb=[0,0,0])\n",
    "l2 = buildLayer(linearf,dlinearf,1,3,w=[1,1,1],wb=0)\n",
    "myfnn = buildNetwork([l1,l2],cost,dcost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. error after epoch 1 : 3.6532211152267577\n",
      "Avg. error after epoch 2 : 2.0839147371036453\n",
      "Avg. error after epoch 3 : 1.9965460916812372\n",
      "Avg. error after epoch 4 : 1.9718232202297679\n",
      "Avg. error after epoch 5 : 1.952221707019908\n",
      "Avg. error after epoch 6 : 1.9333863621103424\n",
      "Avg. error after epoch 7 : 1.9148178098408046\n",
      "Avg. error after epoch 8 : 1.8964517283157738\n",
      "Avg. error after epoch 9 : 1.878277578397346\n",
      "Avg. error after epoch 10 : 1.8602916021318865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10826685330049687"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================\n",
    "# Usage of the FNN\n",
    "# ==================================\n",
    "\n",
    "xtrain = [2 1; 3 3; 4 5; 6 6]\n",
    "ytrain = [10,21,32,42]\n",
    "ytrain = [14,21,28,42]\n",
    "xtest  = [1 1; 2 2; 3 3; 5 5; 10 10]\n",
    "ytest  = [7,14,21,35,70]\n",
    "\n",
    "train!(myfnn,xtrain,ytrain,epochs=10,η=0.001,rshuffle=false) # 1.86\n",
    "errors(myfnn,xtest,ytest) # 0.108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. error after epoch 1 : 345.6182477137603\n",
      "Avg. error after epoch 100 : 60.22718764438183\n",
      "Avg. error after epoch 200 : 25.444742688740504\n",
      "Avg. error after epoch 300 : 15.02111980018784\n",
      "Avg. error after epoch 400 : 8.881146724106667\n",
      "Avg. error after epoch 500 : 6.938551549006719\n",
      "Avg. error after epoch 600 : 2.436588165877931\n",
      "Avg. error after epoch 700 : 1.1640619486870132\n",
      "Avg. error after epoch 800 : 0.6644898036151755\n",
      "Avg. error after epoch 900 : 0.31492530521812157\n",
      "Avg. error after epoch 1000 : 0.1391197919377811\n",
      "Avg. error after epoch 1100 : 0.06173964396518241\n",
      "Avg. error after epoch 1200 : 0.027690372813541326\n",
      "Avg. error after epoch 1300 : 0.01250239909647183\n",
      "Avg. error after epoch 1400 : 0.005667808731962577\n",
      "Avg. error after epoch 1500 : 0.002575917713355535\n",
      "Avg. error after epoch 1600 : 0.001172595165585495\n",
      "Avg. error after epoch 1700 : 0.0005343414033200432\n",
      "Avg. error after epoch 1800 : 0.00024366253299397243\n",
      "Avg. error after epoch 1900 : 0.000111162235383564\n",
      "Avg. error after epoch 2000 : 5.0729240620829226e-5\n",
      "Avg. error after epoch 2100 : 2.3155187469029223e-5\n",
      "Avg. error after epoch 2200 : 1.0570558842015383e-5\n",
      "Avg. error after epoch 2300 : 4.826005241784079e-6\n",
      "Avg. error after epoch 2400 : 2.2034577251308056e-6\n",
      "Avg. error after epoch 2500 : 1.0060971808174018e-6\n",
      "Avg. error after epoch 2600 : 4.5939628309567974e-7\n",
      "Avg. error after epoch 2700 : 2.0976998700319555e-7\n",
      "Avg. error after epoch 2800 : 9.578662050996918e-8\n",
      "Avg. error after epoch 2900 : 4.37391310409614e-8\n",
      "Avg. error after epoch 3000 : 1.997275697794186e-8\n",
      "Avg. error after epoch 3100 : 9.12026839700327e-9\n",
      "Avg. error after epoch 3200 : 4.164648887265452e-9\n",
      "Avg. error after epoch 3300 : 1.9017348212188988e-9\n",
      "Avg. error after epoch 3400 : 8.684044877230784e-10\n",
      "Avg. error after epoch 3500 : 3.96546864932563e-10\n",
      "Avg. error after epoch 3600 : 1.8107864133234046e-10\n",
      "Avg. error after epoch 3700 : 8.26875451566043e-11\n",
      "Avg. error after epoch 3800 : 3.775835661541927e-11\n",
      "Avg. error after epoch 3900 : 1.7241940499113278e-11\n",
      "Avg. error after epoch 4000 : 7.873344395835329e-12\n",
      "Avg. error after epoch 4100 : 3.59527735019269e-12\n",
      "Avg. error after epoch 4200 : 1.6417444463567926e-12\n",
      "Avg. error after epoch 4300 : 7.496848439997724e-13\n",
      "Avg. error after epoch 4400 : 3.423354857816302e-13\n",
      "Avg. error after epoch 4500 : 1.563238138325249e-13\n",
      "Avg. error after epoch 4600 : 7.138357799210462e-14\n",
      "Avg. error after epoch 4700 : 3.259653525222769e-14\n",
      "Avg. error after epoch 4800 : 1.488485489289782e-14\n",
      "Avg. error after epoch 4900 : 6.797009540413842e-15\n",
      "Avg. error after epoch 5000 : 3.103780143585832e-15\n",
      "Avg. error after epoch 5100 : 1.4173084633496158e-15\n",
      "Avg. error after epoch 5200 : 6.471987794245392e-16\n",
      "Avg. error after epoch 5300 : 2.9553587600718883e-16\n",
      "Avg. error after epoch 5400 : 1.3495325224186554e-16\n",
      "Avg. error after epoch 5500 : 6.162499994628911e-17\n",
      "Avg. error after epoch 5600 : 2.814023852723303e-17\n",
      "Avg. error after epoch 5700 : 1.285000025789579e-17\n",
      "Avg. error after epoch 5800 : 5.8677244420006686e-18\n",
      "Avg. error after epoch 5900 : 2.6794309176495283e-18\n",
      "Avg. error after epoch 6000 : 1.2235658229805171e-18\n",
      "Avg. error after epoch 6100 : 5.587349048065613e-19\n",
      "Avg. error after epoch 6200 : 2.551402095532235e-19\n",
      "Avg. error after epoch 6300 : 1.1652200539912674e-19\n",
      "Avg. error after epoch 6400 : 5.321729012068014e-20\n",
      "Avg. error after epoch 6500 : 2.43061791146966e-20\n",
      "Avg. error after epoch 6600 : 1.1102115696522505e-20\n",
      "Avg. error after epoch 6700 : 5.0661990514923784e-21\n",
      "Avg. error after epoch 6800 : 2.315319189271536e-21\n",
      "Avg. error after epoch 6900 : 1.0590817314000296e-21\n",
      "Avg. error after epoch 7000 : 4.812905069347621e-22\n",
      "Avg. error after epoch 7100 : 2.1996847747304875e-22\n",
      "Avg. error after epoch 7200 : 1.0001531334988803e-22\n",
      "Avg. error after epoch 7300 : 4.598441557121892e-23\n",
      "Avg. error after epoch 7400 : 2.1159080029758102e-23\n",
      "Avg. error after epoch 7500 : 9.322539269000719e-24\n",
      "Avg. error after epoch 7600 : 4.160118331542255e-24\n",
      "Avg. error after epoch 7700 : 1.8710711765315826e-24\n",
      "Avg. error after epoch 7800 : 8.056951969384282e-25\n",
      "Avg. error after epoch 7900 : 4.069654523944678e-25\n",
      "Avg. error after epoch 8000 : 1.4324609633546265e-25\n",
      "Avg. error after epoch 8100 : 9.565135691031073e-26\n",
      "Avg. error after epoch 8200 : 9.337273218557984e-26\n",
      "Avg. error after epoch 8300 : 8.750991793797728e-26\n",
      "Avg. error after epoch 8400 : 8.39363780373261e-26\n",
      "Avg. error after epoch 8500 : 7.975383751784429e-26\n",
      "Avg. error after epoch 8600 : 8.069297642550991e-26\n",
      "Avg. error after epoch 8700 : 7.435211246934342e-26\n",
      "Avg. error after epoch 8800 : 6.690487109360445e-26\n",
      "Avg. error after epoch 8900 : 6.937242800513578e-26\n",
      "Avg. error after epoch 9000 : 6.455564331785628e-26\n",
      "Avg. error after epoch 9100 : 5.948681757135867e-26\n",
      "Avg. error after epoch 9200 : 6.126136017765333e-26\n",
      "Avg. error after epoch 9300 : 5.538750187737768e-26\n",
      "Avg. error after epoch 9400 : 5.1272803395744883e-26\n",
      "Avg. error after epoch 9500 : 4.9209537698139327e-26\n",
      "Avg. error after epoch 9600 : 4.7751328314838287e-26\n",
      "Avg. error after epoch 9700 : 4.425982994833009e-26\n",
      "Avg. error after epoch 9800 : 4.393994685126297e-26\n",
      "Avg. error after epoch 9900 : 4.402514382902684e-26\n",
      "Avg. error after epoch 10000 : 4.068037359088975e-26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.22984802594547"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtanh(x)    = 1-tanh(x)^2\n",
    "l1 = buildLayer(tanh,dtanh,3,2)\n",
    "l2 = buildLayer(linearf,dlinearf,1,3)\n",
    "myfnn2 = buildNetwork([l1,l2],cost,dcost)\n",
    "\n",
    "train!(myfnn2,xtrain,ytrain,epochs=10000,η=0.001,rshuffle=false) # 0.011\n",
    "errors(myfnn2,xtest,ytest) # 76.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
